---
title: CIFAR-FS
layout: leaderboard
---


## CIFAR-FS Leaderboard (5-class)

#### [Edit this leaderboard](https://github.com/yaoyao-liu/few-shot-classification-leaderboard/edit/main/cifarfs.md)

Method   | Venue | Year | Backbone   | Setting | 1-shot      | 5-shot   | Code | Reported by 
------- | ------ | ---- | --------   | -----    | -----   | -----    | ---- | ----
[FewTURE](https://arxiv.org/pdf/2206.07267.pdf) | NeurIPS | 2022 | Swin-Tiny | Inductive | 77.76 ± 0.81 |  88.90 ± 0.59 | [\[PyTorch\]](https://github.com/mrkshllr/FewTURE) | [\[Source\]](https://arxiv.org/pdf/2206.07267.pdf)
[FewTURE](https://arxiv.org/pdf/2206.07267.pdf) | NeurIPS | 2022 | ViT-Small | Inductive | 76.10 ± 0.88 |  86.14 ± 0.64 | [\[PyTorch\]](https://github.com/mrkshllr/FewTURE) | [\[Source\]](https://arxiv.org/pdf/2206.07267.pdf)
[HCTransformers](https://arxiv.org/pdf/2203.09064v1.pdf) | CVPR | 2022 | ViT-S | Inductive | 78.89 ± 0.18 | 90.50 ± 0.09 | [\[PyTorch\]](https://github.com/StomachCold/HCTransformers) | [\[Source\]](https://arxiv.org/pdf/2203.09064v1.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 3xResNet-12 | Transductive | 87.16 ± 0.21 |  90.47 ± 0.15 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 2xResNet-12(1/√2) | Transductive | 86.99 ± 0.21 | 90.20 ± 0.15 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 3xResNet-12 | Inductive |  76.20 ± 0.2 |  89.00 ± 0.14 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[EASY](https://arxiv.org/pdf/2201.09699.pdf) | arXiv |  2022 | 2xResNet-12(1/√2) | Inductive |  75.24 ± 0.2 |  88.38 ± 0.1 |  [\[PyTorch\]](https://github.com/ybendou/easy) |  [\[Source\]](https://arxiv.org/pdf/2201.09699.pdf)
[MetaOptNet](https://arxiv.org/pdf/1904.03758.pdf)     | CVPR   | 2019 | ResNet-12  | Inductive | 72.0 ± 0.7    | 84.2 ± 0.5     | [\[PyTorch\]](https://github.com/kjunelee/MetaOptNet) | [\[Source\]](https://arxiv.org/pdf/1904.03758.pdf)
[Dhillon et al.](https://openreview.net/pdf?id=rylXBkrYDS) | ICLR | 2020 | WRN-28-10 | Transductive |  76.58 ± 0.68 | 85.79 ± 0.50 | None | [\[Source\]](https://openreview.net/pdf?id=rylXBkrYDS)
[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | WRN-28-10 | Transductive |  80.0 ± 0.6 | 85.3 ± 0.4 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
[SIB](https://openreview.net/pdf?id=Hkg-xgrYvH) | ICLR | 2020 | 4CONV | Transductive |  68.7 ± 0.6 | 77.1 ± 0.4 | [\[PyTorch\]](https://github.com/hushell/sib_meta_learn) | [\[Source\]](https://openreview.net/pdf?id=Hkg-xgrYvH)
[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV | Inductive | 55.14 ± 0.48 | 71.66 ± 0.39 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
[Ravichandran et al.](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Inductive | 69.15  | 84.70 | None| [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Ravichandran_Few-Shot_Learning_With_Embedded_Class_Models_and_Shot-Free_Meta_Training_ICCV_2019_paper.pdf)
[TEAM](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | ResNet-12 | Transductive |  70.43  |  81.25 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf)
[TEAM](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf) | ICCV | 2019 | 4CONV | Transductive |  64.07 |  79.05 | None | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Qiao_Transductive_Episodic-Wise_Adaptive_Metric_for_Few-Shot_Learning_ICCV_2019_paper.pdf)
[CC+rot](https://openaccess.thecvf.com/content_ICCV_2019/papers/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.pdf) | ICCV | 2019 | WRN-28-10 | Semi-supervised | 75.38 ± 0.31 | 87.25 ± 0.21| [\[PyTorch\]](https://github.com/valeoai/BF3S) | [\[Source\]](https://openaccess.thecvf.com/content_ICCV_2019/papers/Gidaris_Boosting_Few-Shot_Visual_Learning_With_Self-Supervision_ICCV_2019_paper.pdf)
[DSN-MR](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Transductive |  75.6 ± 0.9 | 86.2 ± 0.6 | [\[PyTorch\]](https://github.com/chrysts/dsn_fewshot) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Simon_Adaptive_Subspaces_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | ResNet-12 | Transductive |  77.9 ± 0.5 | 90.2 ± 0.4  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[DPGN](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 | 4CONV | Transductive |  76.4 ± 0.5 | 88.4 ± 0.4  | [\[PyTorch\]](https://github.com/megvii-research/DPGN) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Yang_DPGN_Distribution_Propagation_Graph_Network_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-12 | Transductive |  73.97 | 84.13 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[ICI](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf) | CVPR | 2020 |  ResNet-12 | Semi-supervised | 76.51 | 84.32 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://openaccess.thecvf.com/content_CVPR_2020/papers/Wang_Instance_Credibility_Inference_for_Few-Shot_Learning_CVPR_2020_paper.pdf)
[J. Kim et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460579.pdf) | ECCV | 2020 |  ResNet-12 | Inductive |  73.51 ± 0.92 |  85.65 ± 0.65 | [\[PyTorch\]](https://github.com/jaekyeom/MABAS) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123460579.pdf)
[Y. Tian et al.](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf) | ECCV | 2020 | ResNet-12 | Inductive | 73.9 ± 0.8 | 86.9 ± 0.5 | [\[PyTorch\]](https://github.com/WangYueFt/rfs) | [\[Source\]](http://www.ecva.net/papers/eccv_2020/papers_ECCV/papers/123590256.pdf)
[ICI v2](https://arxiv.org/pdf/2007.08461.pdf) | TPAMI | 2021 |  ResNet-12 | Semi-supervised |  80.74 ± 0.61  | 87.16 ± 0.36 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://arxiv.org/pdf/2007.08461.pdf)
[ICI v2](https://arxiv.org/pdf/2007.08461.pdf) | TPAMI | 2021 |  ResNet-12 | Transductive |  79.19 ± 0.63 | 86.66 ± 0.36 | [\[PyTorch\]](https://github.com/Yikai-Wang/ICI-FSL) | [\[Source\]](https://arxiv.org/pdf/2007.08461.pdf)
[ConstellationNet](https://openreview.net/pdf?id=vujTf_I8Kmc)     | ICLR   | 2021 | ResNet-12   | Inductive | 75.4 ± 0.2   | 86.8 ± 0.2     | None | [\[Source\]](https://openreview.net/pdf?id=vujTf_I8Kmc)
[ConstellationNet](https://openreview.net/pdf?id=vujTf_I8Kmc)     | ICLR   | 2021 | 4CONV   | Inductive | 69.3 ± 0.3   | 82.7 ± 0.2     | None | [\[Source\]](https://openreview.net/pdf?id=vujTf_I8Kmc)
[PT+MAP](https://arxiv.org/pdf/2006.03806v3.pdf) | arXiv | 2021 | WRN | Inductive | 74.64 ± 0.21 | 87.64 ± 0.15  | [\[PyTorch\]](https://github.com/yhu01/PT-MAP) | [\[Source\]](https://arxiv.org/pdf/2006.03806v3.pdf)
[PT+MAP](https://arxiv.org/pdf/2006.03806v3.pdf) | arXiv | 2021 | WRN | Transductive |  87.69 ± 0.23  | 90.68 ± 0.15 | [\[PyTorch\]](https://github.com/yhu01/PT-MAP) | [\[Source\]](https://arxiv.org/pdf/2006.03806v3.pdf)
[ECKPN](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | 4CONV  | Transductive | 77.5±0.4  | 89.1±0.5 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf)
[ECKPN](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12  | Transductive | 79.2±0.4   | 91.0±0.5  | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Chen_ECKPN_Explicit_Class_Knowledge_Propagation_Network_for_Transductive_Few-Shot_Learning_CVPR_2021_paper.pdf)
[Rizve et al.](https://openaccess.thecvf.com/content/CVPR2021/papers/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.pdf) | CVPR | 2021 | ResNet-12 | Inductive | 77.87±0.85 |  89.74±0.57 | None | [\[Source\]](https://openaccess.thecvf.com/content/CVPR2021/papers/Rizve_Exploring_Complementary_Strengths_of_Invariant_and_Equivariant_Representations_for_Few-Shot_CVPR_2021_paper.pdf)
[PLCM](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Pseudo-Loss_Confidence_Metric_for_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Semi-supervised | 77.62±1.15 | 86.13±0.67 | None | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Huang_Pseudo-Loss_Confidence_Metric_for_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[Curvature Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 (64-160-320-640) | Inductive | 73.0±0.7 | 85.8±0.5 |  [\[PyTorch\]](https://github.com/ZhiGaomcislab/CurvatureGeneration_FSL)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[Curvature Generation](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 (64-160-320-640) | Transductive | 76.8±0.7 | 86.4±0.5 |  [\[PyTorch\]](https://github.com/ZhiGaomcislab/CurvatureGeneration_FSL)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Gao_Curvature_Generation_in_Curved_Spaces_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[TPMN](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Task-Aware_Part_Mining_Network_for_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 75.5±0.9 | 87.2±0.6 | None | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Wu_Task-Aware_Part_Mining_Network_for_Few-Shot_Learning_ICCV_2021_paper.pdf)
[RENet](https://openaccess.thecvf.com/content/ICCV2021/papers/Kang_Relational_Embedding_for_Few-Shot_Classification_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 74.51±0.46 | 86.60±0.32 |  [\[PyTorch\]](https://github.com/dahyun-kang/renet)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Kang_Relational_Embedding_for_Few-Shot_Classification_ICCV_2021_paper.pdf)
[BML](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Binocular_Mutual_Learning_for_Improving_Few-Shot_Classification_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive |  73.45±0.47 | 88.04±0.33 |  [\[PyTorch\]](https://github.com/ZZQzzq/BML)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhou_Binocular_Mutual_Learning_for_Improving_Few-Shot_Classification_ICCV_2021_paper.pdf)
[MetaQDA](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Shallow_Bayesian_Meta_Learning_for_Real-World_Few-Shot_Recognition_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Inductive | 75.83±0.88 | 88.79±0.75 |  [\[PyTorch\]](https://github.com/Open-Debin/Bayesian_MQDA)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Shallow_Bayesian_Meta_Learning_for_Real-World_Few-Shot_Recognition_ICCV_2021_paper.pdf)
[Meta Navigator](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Meta_Navigator_Search_for_a_Good_Adaptation_Policy_for_Few-Shot_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Inductive | 74.63±0.91 | 86.45±0.59 |  None  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Zhang_Meta_Navigator_Search_for_a_Good_Adaptation_Policy_for_Few-Shot_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Transductive |  77.14±0.95 | 85.23±0.5 |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Transductive |  86.51±0.75 | 90.60±0.48 |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | ResNet-12 | Semi-supervised | 78.57±0.80 | 85.84±0.56 |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[iLPC](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Semi-supervised  | 90.34±0.50 | 91.69±0.55 |  [\[PyTorch\]](https://github.com/MichalisLazarou/iLPC)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lazarou_Iterative_Label_Cleaning_for_Transductive_and_Semi-Supervised_Few-Shot_Learning_ICCV_2021_paper.pdf)
[AIM](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Few-Shot_and_Continual_Learning_With_Attentive_Independent_Mechanisms_ICCV_2021_paper.pdf) | ICCV | 2021 | WRN-28-10 | Transductive |  80.20±0.55 | 87.34±0.36 |  [\[PyTorch\]](https://github.com/huang50213/AIM-Fewshot-Continual)  | [\[Source\]](https://openaccess.thecvf.com/content/ICCV2021/papers/Lee_Few-Shot_and_Continual_Learning_With_Attentive_Independent_Mechanisms_ICCV_2021_paper.pdf)
[Wang et al.](https://papers.nips.cc/paper/2021/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf) | NeurIPS | 2021 | 4CONV | Inductive | 64.17±0.31 | 78.42±0.26 | None | [\[Source\]](https://papers.nips.cc/paper/2021/file/6e2713a6efee97bacb63e52c54f0ada0-Paper.pdf)
[NCA nearest centroid](https://papers.nips.cc/paper/2021/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Inductive | 72.49±0.12 | 85.15±0.09 | [\[PyTorch\]](https://github.com/fiveai/on-episodes-fsl) | [\[Source\]](https://papers.nips.cc/paper/2021/file/cdfa4c42f465a5a66871587c69fcfa34-Paper.pdf)
[SSR](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf) | NeurIPS | 2021 | ResNet-12 | Transductive | 76.8±0.6 | 83.7±0.4 | [\[PyTorch\]](https://github.com/XiSHEN0220/SSR) | [\[Source\]](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf)
[SSR](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf) | NeurIPS | 2021 | WRN-28-10 | Transductive | 81.6±0.6 | 86.0±0.4 | [\[PyTorch\]](https://github.com/XiSHEN0220/SSR) | [\[Source\]](https://papers.nips.cc/paper/2021/file/d9fc0cdb67638d50f411432d0d41d0ba-Paper.pdf)
